# ğŸ§  AnÃ¡lise Computacional da ConvergÃªncia de Otimizadores de IA

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![NumPy](https://img.shields.io/badge/NumPy-1.24+-orange.svg)](https://numpy.org/)
[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.7+-green.svg)](https://matplotlib.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> **Trabalho de ConclusÃ£o de Curso (TCC)**  
> **Universidade Federal do MaranhÃ£o (UFMA)**  
> **Bacharelado Interdisciplinar em CiÃªncia e Tecnologia**  
> **Autor:** JoÃ£o Victor Lima Azevedo  
> **Orientador:** Prof. Dr. Jadevilson Cruz Ribeiro

---

## ğŸ“‹ Sobre o Projeto

Este projeto implementa e compara empiricamente dois algoritmos fundamentais de otimizaÃ§Ã£o matemÃ¡tica aplicados a problemas de InteligÃªncia Artificial:

- **Gradiente Descendente (GD)** - MÃ©todo de primeira ordem
- **Newton-Raphson (NR)** - MÃ©todo de segunda ordem

### ğŸ¯ Objetivo

Investigar quantitativamente as diferenÃ§as de **velocidade**, **eficiÃªncia** e **robustez** entre os mÃ©todos usando a **FunÃ§Ã£o de Rosenbrock** como benchmark.

### ğŸ† Resultado Principal

**Newton-Raphson foi 798x mais rÃ¡pido** que Gradiente Descendente, convergindo em apenas **6 iteraÃ§Ãµes** versus **32.076 iteraÃ§Ãµes**.

---

## ğŸ“Š Resultados Visuais

### SuperfÃ­cie 3D com TrajetÃ³rias
![SuperfÃ­cie 3D](docs/images/preview_3d.png)
*Gradiente Descendente (vermelho) faz zig-zag; Newton-Raphson (azul) segue direto ao mÃ­nimo*

### ComparaÃ§Ã£o de Desempenho
| MÃ©trica | Gradiente Descendente | Newton-Raphson | Vantagem NR |
|---------|----------------------|----------------|-------------|
| **IteraÃ§Ãµes** | 32.076 | 6 | **5.346x** |
| **Tempo Total** | 214.79 ms | 0.27 ms | **798x** âš¡ |
| **Erro Final** | 2.50Ã—10â»â¶ | 1.89Ã—10â»Â¹Â¹ | **10.000x** |
| **Custo/IteraÃ§Ã£o** | 6.70 Î¼s | 44.83 Î¼s | 6.7x mais caro |

---

## ğŸš€ InÃ­cio RÃ¡pido

### PrÃ©-requisitos

```bash
Python 3.8+
```

### InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone https://github.com/SEU_USUARIO/tcc-otimizadores-ia.git
cd tcc-otimizadores-ia

# Instale as dependÃªncias
pip install -r requirements.txt
```

### Executar

```bash
# Rodar TUDO (experimentos + grÃ¡ficos)
python executar_completo.py

# Ou rodar apenas os experimentos
python src/experiments.py

# Ou gerar apenas os grÃ¡ficos (se jÃ¡ tem dados)
python src/visualization.py
```

### SaÃ­da

```
results/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ resultados_experimentos.json
â””â”€â”€ plots/
    â”œâ”€â”€ grafico_1_superficie_3d.png
    â”œâ”€â”€ grafico_2_curvas_nivel.png
    â”œâ”€â”€ grafico_3_convergencia.png
    â”œâ”€â”€ grafico_4_comparacao_desempenho.png
    â””â”€â”€ grafico_5_sensibilidade_alpha.png
```

---

## ğŸ“ Estrutura do Projeto

```
tcc-otimizadores-ia/
â”‚
â”œâ”€â”€ README.md                      # Este arquivo
â”œâ”€â”€ requirements.txt               # DependÃªncias Python
â”œâ”€â”€ LICENSE                        # LicenÃ§a MIT
â”œâ”€â”€ .gitignore                     # Arquivos ignorados pelo Git
â”‚
â”œâ”€â”€ executar_completo.py           # ğŸ¯ SCRIPT PRINCIPAL
â”‚
â”œâ”€â”€ src/                           # CÃ³digo-fonte
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ rosenbrock.py              # FunÃ§Ã£o de teste + derivadas
â”‚   â”œâ”€â”€ optimizers.py              # ImplementaÃ§Ã£o GD e NR
â”‚   â”œâ”€â”€ experiments.py             # 4 experimentos do TCC
â”‚   â””â”€â”€ visualization.py           # GeraÃ§Ã£o de grÃ¡ficos
â”‚
â”œâ”€â”€ docs/                          # DocumentaÃ§Ã£o
â”‚   â”œâ”€â”€ TCC_Completo.pdf           # (Opcional) PDF do TCC
â”‚   â””â”€â”€ images/                    # Imagens de exemplo
â”‚
â”œâ”€â”€ results/                       # Resultados (criado automaticamente)
â”‚   â”œâ”€â”€ data/                      # Dados JSON
â”‚   â””â”€â”€ plots/                     # GrÃ¡ficos PNG
â”‚
â””â”€â”€ tests/                         # (Futuro) Testes unitÃ¡rios
    â””â”€â”€ test_rosenbrock.py
```

---

## ğŸ§ª Experimentos Implementados

### Experimento 1: ConvergÃªncia BÃ¡sica
Compara comportamento padrÃ£o dos dois mÃ©todos partindo de `(-1.2, 1.0)`.

**Resultado:** NR converge em 6 iteraÃ§Ãµes vs GD em 32.076.

### Experimento 2: Sensibilidade ao Learning Rate
Testa GD com 6 valores de Î± âˆˆ `{0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1}`.

**Resultado:** Apenas Î±=0.001 converge adequadamente; valores fora dessa janela divergem ou sÃ£o lentos.

### Experimento 3: Robustez a Pontos Iniciais
Avalia convergÃªncia de 5 pontos iniciais diferentes.

**Resultado:** NR converge de todos (100%); GD falha em 1 ponto (80%).

### Experimento 4: Trade-off Computacional
Mede tempo de execuÃ§Ã£o com precisÃ£o.

**Resultado:** NR 6.7x mais caro por iteraÃ§Ã£o, mas 798x mais rÃ¡pido no total.

---

## ğŸ“ˆ GrÃ¡ficos Gerados

### 1. SuperfÃ­cie 3D (`grafico_1_superficie_3d.png`)
SuperfÃ­cie tridimensional da funÃ§Ã£o de Rosenbrock com trajetÃ³rias dos algoritmos.

### 2. Curvas de NÃ­vel (`grafico_2_curvas_nivel.png`)
Vista superior evidenciando o "zig-zag" do GD vs caminho direto do NR.

### 3. ConvergÃªncia (`grafico_3_convergencia.png`)
AnÃ¡lise quantitativa: erro e valor da funÃ§Ã£o vs iteraÃ§Ã£o (escala log).

### 4. ComparaÃ§Ã£o de Desempenho (`grafico_4_comparacao_desempenho.png`)
4 subplots comparando iteraÃ§Ãµes, tempo, erro e custo por iteraÃ§Ã£o.

### 5. Sensibilidade ao Î± (`grafico_5_sensibilidade_alpha.png`)
Mostra janela estreita de valores de Î± que funcionam para GD.

---

## ğŸ”¬ Metodologia

### FunÃ§Ã£o de Teste: Rosenbrock

```python
f(x, y) = (1 - x)Â² + 100(y - xÂ²)Â²
```

- **MÃ­nimo global:** `(1, 1)` com `f(1,1) = 0`
- **CaracterÃ­stica:** Vale estreito e curvo ("banana valley")
- **NÃºmero de condiÃ§Ã£o:** Îº â‰ˆ 2500 (mal condicionado)

### ImplementaÃ§Ã£o

- **Linguagem:** Python 3.8+
- **Bibliotecas:** NumPy, Matplotlib
- **Abordagem:** ImplementaÃ§Ã£o *from scratch* (sem bibliotecas prontas de otimizaÃ§Ã£o)
- **Derivadas:** AnalÃ­ticas (nÃ£o numÃ©ricas)

### CritÃ©rio de ConvergÃªncia

```python
||âˆ‡f(x)|| < 10â»â¶  ou  max_iteraÃ§Ãµes atingido
```

---

## ğŸ“š FundamentaÃ§Ã£o TeÃ³rica

### Gradiente Descendente

```python
x_{k+1} = x_k - Î±Â·âˆ‡f(x_k)
```

- **Taxa de convergÃªncia:** Linear (geomÃ©trica)
- **Custo por iteraÃ§Ã£o:** O(n)
- **Vantagem:** Simples, baixo custo
- **Desvantagem:** ConvergÃªncia lenta, sensÃ­vel a Î±

### Newton-Raphson

```python
H(x_k)Â·d = -âˆ‡f(x_k)
x_{k+1} = x_k + d
```

- **Taxa de convergÃªncia:** QuadrÃ¡tica
- **Custo por iteraÃ§Ã£o:** O(nÂ³)
- **Vantagem:** ConvergÃªncia dramÃ¡tica
- **Desvantagem:** Caro computacionalmente, requer Hessiana

---

## ğŸ“ ConclusÃµes

### 3 Insights Fundamentais

1. **ConvergÃªncia QuadrÃ¡tica Ã© Poderosa**  
   798x de vantagem nÃ£o Ã© marginal - Ã© transformacional.

2. **Curvatura Vale o Custo**  
   6.7x mais caro por iteraÃ§Ã£o, mas 5.346x menos iteraÃ§Ãµes = vantagem lÃ­quida massiva.

3. **Contexto Determina Escolha**  
   - **Use NR:** Baixa dimensÃ£o (n < 1000), Hessiana tratÃ¡vel, precisÃ£o crÃ­tica
   - **Use GD:** Alta dimensÃ£o (n > 1000), memÃ³ria limitada, Deep Learning

---

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python 3.8+** - Linguagem de programaÃ§Ã£o
- **NumPy 1.24+** - ComputaÃ§Ã£o numÃ©rica
- **Matplotlib 3.7+** - VisualizaÃ§Ã£o de dados
- **JSON** - Armazenamento de resultados

---

## ğŸ¤ Como Contribuir

1. Fork este repositÃ³rio
2. Crie uma branch: `git checkout -b feature/nova-funcionalidade`
3. Commit suas mudanÃ§as: `git commit -m 'Adiciona nova funcionalidade'`
4. Push para a branch: `git push origin feature/nova-funcionalidade`
5. Abra um Pull Request

### Ideias para ContribuiÃ§Ã£o

- [ ] Implementar mÃ©todos Quasi-Newton (BFGS, L-BFGS)
- [ ] Adicionar otimizadores modernos (Adam, RMSprop)
- [ ] Estender para Rosenbrock N-dimensional
- [ ] Testar em problemas reais de ML
- [ ] Adicionar testes unitÃ¡rios
- [ ] Criar interface web interativa

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

---

## ğŸ‘¤ Autor

**JoÃ£o Victor Lima Azevedo**

- GitHub: [@jvictorlima](https://github.com/jvictorlima)
- LinkedIn: [linkedin.com/in/joaovictorlima](https://linkedin.com/in/joaovictorlima)
- Email: joao.victor@discente.ufma.br

**Orientador:** Prof. Dr. Jadevilson Cruz Ribeiro  
**InstituiÃ§Ã£o:** Universidade Federal do MaranhÃ£o (UFMA)  
**Curso:** Bacharelado Interdisciplinar em CiÃªncia e Tecnologia  
**Ano:** 2025

---

## ğŸ™ Agradecimentos

- Prof. Dr. Jadevilson Cruz Ribeiro pela orientaÃ§Ã£o fundamental
- UFMA pelo suporte institucional
- Comunidade open-source (Python, NumPy, Matplotlib)

---

## ğŸ“– CitaÃ§Ã£o

Se vocÃª usar este cÃ³digo em sua pesquisa, por favor cite:

```bibtex
@misc{azevedo2025otimizadores,
  author = {Azevedo, JoÃ£o Victor Lima},
  title = {AnÃ¡lise Computacional da ConvergÃªncia de Otimizadores de IA: 
           Newton-Raphson vs. Gradiente Descendente},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/jvictorlima/tcc-otimizadores-ia}}
}
```

---

## ğŸ“ Contato

Para dÃºvidas, sugestÃµes ou colaboraÃ§Ãµes, abra uma [Issue](https://github.com/jvictorlima/tcc-otimizadores-ia/issues) ou envie um email.

---

<div align="center">

**â­ Se este projeto foi Ãºtil, deixe uma estrela! â­**

Desenvolvido com ğŸ’™ por [JoÃ£o Victor Lima Azevedo](https://github.com/jvictorlima)

</div>
